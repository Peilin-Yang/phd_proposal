%
% This is Chapter 2 file (chap2.tex)
%
\chapter{Related Work}

In this thesis, we investigate the impact of context in different IR systems. 
Specifically, two kinds of systems are mainly explored. 
The first one is the contextual suggestion. The key of solving this 
problem is to build the user profile based on the user preference history. 
We include user preference as part of the context of this problem as 
this is the long term and thus static information need of the user.
The other system is the unified evaluation system where the context is the 
evaluation system itself on which the comparison of different ranking models 
relies. With the help of the unified evaluation system we tried to 
provide analytical performance upper bound for bag-of-terms ranking models 
of which the context here are the statistics used by the models. 
Extensive work have been done on solving these research questions. 
We now survey the related work in the literature and discuss the differences
with our proposed approaches in detail.


\section{Contextual Suggestion}

The problem of contextual suggestion was first introduced at
TREC in 2012, and the track has been running in the past 
four years \cite{adriel:overview, adriel:overview2013, adriel:overview2014, 
adriel:overview2015}. 
Although the details of the track varied, the task remains 
the same.  Given a user's preferences on a set of example 
suggestions and a context, track participants
are expected to return a ranked list of new suggestions that 
are likely to satisfy both the user preferences (based on their preferences
on the example suggestions) as well as the contexts such as 
geotemporal locations. Each example suggestion includes a title, 
description and an associated URL. For each user, we know their 
preferences on part or all of the example suggestions. 

Most TREC participants retrieved candidate suggestions from various 
online services such as 
Google Place %\footnote{\url{https://developers.google.com/places/}}  
or Yelp %\footnote{\url{http://www.yelp.com}} 
based on the geographical context and then use some heuristics, 
e.g. nightclub will not be shown if the temporal context is in the morning, 
to filter out the suggestions that do not match the temporal contexts 
\cite{adriel:overview, adriel:overview2013}.
After that, the task is to retrieve useful suggestions based on 
user preferences. Most participants formulated the task as a 
content-based recommendation problem 
\cite{irit:treccs2012, georgetown:treccs2012, udel:treccs2012, 
udben:treccs2012, isi:treccs2013, up:treccs2013, udel:treccs2014, 
cmu:treccs2014, bjut:treccs2014, rama:treccs2014, glasgow:treccs2014}. 
A common strategy adopted by top-ranked participants of TREC is to 
estimate a user profile based on the example suggestions and then rank 
candidate suggestions based on their similarities to the user profile. 
The basic assumption is that a user would prefer suggestions that are 
similar to those example suggestions liked by the user.  
% Various types of information about the suggestions have been used
% to estimate user profiles which include 
% the description of the places \cite{udel:treccs2012,irit:treccs2012,up:treccs2013}, 
% the categories of the places \cite{rama:treccs2014,bjut:treccs2014,glasgow:treccs2014,
% georgetown:treccs2012,uamst:treccs2013},
% and the web sites of the places \cite{udel:treccs2012,irit:treccs2012,up:treccs2013}.

There are some studies that used terms from the description of the places or 
the web pages of the example suggestions to construct user profiles, 
and then various similarity measures are used to rank the candidates
\cite{udel:treccs2012,irit:treccs2012,up:treccs2013}. 
A few studies also explored the use of category information for user profiling 
and candidate ranking. For example, 
Li and Alonso \cite{rama:treccs2014} utilized the accumulative category 
scores to model both user and candidate profiles, and then use 
the full range cosine similarity between the two profiles for candidate ranking. 
Li et al. \cite{bjut:treccs2014} leveraged how likely each popular category is 
liked/disliked by users to construct user profiles, and the candidate ranking is 
to favor suggestions from a user's favorite categories. 
McCreadie et al. \cite{glasgow:treccs2014} proposed to rank the 
candidates by comparing two trees of finer-grained categories between 
user profile and candidate profile using a tree-matching technique. 
Diversification is then applied so that the categories of top ranked 
candidates are normalized.
Yates et al. \cite{georgetown:treccs2012} proposed to 
recommend the candidates which are proportional to the number of 
example suggestions in each category.
Koolen et al. \cite{uamst:treccs2013} applied a similar method 
with a major modification of 
retrieving the category information from 
Wikitravel\footnote{http://www.wikitravel.org/}.

Although we also use category and description of example suggestions 
to build user profile, we propose to leverage text reviews about the 
example suggestion to estimate the user profile which is unseen from 
previous works.

\subsection{Recommendation Systems} 

The problem of contextual suggestion is also similar to 
collaborative filtering \cite{Su:2009:SCF:1592474.1722966}. 
Collaborative filtering assumes that similar users
would share similar ratings, and focuses on predicting
the user rating based on such an assumption.
It often requires a large number of past user preferences to be more accurate 
and sometimes it may suffer from data sparsity problem which is known 
as the cold start problem \cite{Schein:2002:MMC:564376.564421}.
In order to solve the data sparsity problem, reviews were 
incorporated to improve the performance. 
Hariri et al. \cite{hariricontext} inferred the context or the intent 
of the trip by analyzing reviews. In particular, they used latent Dirichlet 
Allocation to identify the topics from the reviews, and the final 
ranking scores are generated based on both the context scores as well 
as the scores generated by traditional collaborative filtering methods.  
Jakob et al. \cite{Jakob:2009:BSE:1651461.1651473} proposed to 
cluster the features and then apply natural language processing 
techniques to identify the polarity of the opinions.
A few studies also focused on leveraging Location Based Social Network 
to solve the data sparsity problem. 
Noulas et al. \cite{Noulas:2012:RWA:2411131.2411620} applied random 
walk based on latent space models and computed a variety of 
similarity criteria with venue's visit frequencies on the 
location based social newtowkr. 
Bao et al \cite{Bao:2012:LPR:2424321.2424348} proposed to first 
constructing a weighted category hierarchy and then identify  
local experts for each category. The local experts are then matched 
to a given user and the score of the candidate is inferred based on the 
opinions of the local experts.


Our work is also related to other studies that utilized reviews to improve 
the performance of recommendation systems 
\cite{Qumsiyeh:2012:PRM:2348283.2348349, SanPedro:2012:LUC:2187836.2187896, 
Raghavan:2012:RQA:2365952.2365978,hariricontext,Levi:2012:FNH:2365952.2365977}.
Raghavan et al. \cite{Raghavan:2012:RQA:2365952.2365978} proposed to 
use the helpfulness, features from the text reviews and 
the meta-data (average rating, average length of text reviews and etc.) 
of the opinions to train a regression model in order to generate a 
quality score for each opinion. The quality score is then 
incorporated into the probabilistic matrix factorization as an 
inverse factor which affects the variance of the prediction from the 
mean of the factor model.
Levi et al. \cite{Levi:2012:FNH:2365952.2365977} extended 
this study and analyzed the review texts to get the intent, 
features and the ratings for each feature. 
Qumsiyeh and Ng \cite{Qumsiyeh:2012:PRM:2348283.2348349} explored the 
aspects in the reviews and computed the probability of each genres 
(categories) in each rating level. Their work is limited to the 
applications in multimedia domains, and the genres of each type of 
media is pre-defined. 

Our work is different from these previous studies in the following 
aspects. First, our focus is to directly use reviews to model user 
profile while previous studies mainly used reviews to predict the 
rating quality or the user intent. Second, existing studies on 
collaborative filtering were often evaluated on only specific applications, 
e.g., movies, hotels, and it is unclear how those methods could 
be generalized to other domains. In contrast, our proposed method 
is not limited to any specific domains and can be applied to 
a more general problem set up.

\subsection{Text Summarization} 

The summary generation of our work is related to automatic text 
summarization. 
Automatic text summarization has been well studied for traditional 
documents such as scientific documents and news articles \cite{Radev:02}. 
In particular, previous work has studied various problems in this 
area including extractive summarization, abstractive summarization, 
single-document summarization and multiple-document summarization \cite{Das:07}. 
More recently, there have been effort on opinion mining and 
summarization \cite{Pang:2008:OMS:1454711.1454712,
Pak10, Pang:2004:SES:1218955.1218990,
mani1999advances,mani2001automatic,knight2002summarization,
ding2007utility,BACCIANELLA10.769,Chen:2010:AOM:1829879.1829923,
dey2009opinion,journals/sigir/Esuli08}. Most of them involve in the 
finer partition of the reviews and polarity judging of each partition. 
Common strategies include part-of-speech analysis, negation 
identification and etc. 
Unlike the previous effort, we focus on generating a 
{\em personalized} summary for a suggestion. 
Since the information about the suggestion is scattered in 
many places, including description, web sites 
and reviews, the summarization needs to synthesize the information 
from these heterogeneous information sources. Instead of 
extracting the information from a single source, we try to 
leverage one information source to guide the extractive 
summarization process in other sources and then assemble
all the extracted summaries together into a {\em structural} way.  
Another main difference of our work from previous studies is to 
utilize the user profile to generate personalized summaries.  

\section{Unified IR Evaluation System}

There have been significant efforts on developing various web 
services for IR evaluation. 
Lin et al. \cite{conf/ecir/LinCTCCFIMV16} proposed an open-source 
IR reproducibility challenge where they split the IR system into 
pieces of components such as two kinds of tokenization methods and 
four different IR toolkits. 
By easily configuring different combinations of these components, we 
can have a partially filled matrix indicating the performances of 
specific combinations of the components. Such transparent 
experiment set up makes it possible to have a better understanding about 
the impact of different components. 
Gollub et al. \cite{Gollub:2012:OIT:2348283.2348501} described a reference 
implementation of their proposed IR evaluation web service which bears 
the important properties like web dissemination and peer-to-peer 
collaboration. 
Hanbury et al. \cite{Hanbury:2010:ACE:1889174.1889194} reviewed some 
of the existing automated IR evaluation approaches and proposed a 
framework for web service based component-level IR system evaluation. 
Lagun and Agichtein proposed a web service, which enables large scale 
studies of remote users \cite{Lagun:2011:VEL:2009916.2009967}. 
Their system focused on providing a platform that reproduces and 
extends the previous findings on how users interact with the 
search engine especially the search results. 

Our developed systems {\em VIRLab} and {\em RISE} are closely related to 
the ideas of Privacy Preserved Evaluation (PPE) 
\cite{Fang:2014:VWV:2600428.2611178}
and Evaluation as a Service (EaaS) 
\cite{rao:ecir:2015,Lin:2013:ESI:2568388.2568390,Lin:2014:ISE:2567948.2577014}. 
VIRLab \cite{Fang:2014:VWV:2600428.2611178} provides a web service
for users to implement retrieval functions. It is mainly designed
to facilitate teaching IR models. RISE is also designed as a 
web service to provide a unified interface for the users to evaluate their 
models/algorithms. This design enables the system to host the data 
collections instead of shipping the data collections to researchers, 
which can ensure the privacy of the collections. 
Users of RISE can not see the functions implemented by other users. 
The uniqueness of 
{\em RISE} system is that it is specifically designed to facilitate the 
implementation and evaluation of retrieval functions.  

The SIGIR 2015 Workshop on Reproducibility, Inexplicability, and 
Generalizability of Results (RIGOR) \cite{Hopfgartner:2015:REE:2795403.2795416} 
is one of the venues that encourage the study of reproducibility. 
Their reproducibility challenge invited developers 
of 7 open-source search engines to provide baselines for TREC GOV2 
collection.
Trotman et. al. \cite{adcs14} and Muhleisen el. al. \cite{Muhleisen:2014:ODG:2600428.2609460} 
have also tried to reproduce retrieval results for IR models, but the
number of retrieval functions and the number of collections used in these studies 
(1 function 1 collection for \cite{adcs14} and 9 functions  
10 collections for \cite{Muhleisen:2014:ODG:2600428.2609460})
are not as large as what we studied in this paper. 

Compared with the previous studies, our work is different in the following
two aspects. First, the {\em RISE} system is specifically designed for 
the reproducibility study of retrieval models. It hides details about 
collection processing and evaluation, and enables users to focus on 
only the implementation of retrieval models. Due to its flexibility, 
we are able to implement and compare a wide range of retrieval functions 
that were not implemented in any other open-source toolkits. Second, 
our reproducibility study includes more retrieval functions and more data 
collections. The ultimate goal of the {\em RISE} system is to provide 
a complete set of benchmark results of IR models. 


\section{Boundary Theory of Bag-of-Terms Models}

Although there are lots of effective ranking models proposed by researchers, 
there are fewer studies dedicated to the theoretical analysis of their 
performances upper bound. One related domain is the constraint analysis 
\cite{Fang:2004:FSI:1008992.1009004} which proposes formal constraints 
that a reasonable ranking model should bear. Examples of the constraints 
including how should a ranking model incorporate TF, how to regulate 
the interaction of TF and DL, how to penalize long document in the 
collection, etc. The constraint analysis provides a general guide of how 
a reasonable ranking model should be designed. Our work further explores 
this direction by providing the practical performance upper bound as 
well as the optimal parameters which helps to fine tune the constraint 
theory. 

Our estimation method is mostly inspired by the RankNet 
\cite{export:68133,export:132652} and the LambdaRank 
\cite{export:132652,export:81144} which are 
successful in the learning to rank domain. 
In their works they apply the pair-wise documents comparison for a 
specific query which is also adopted by our work.
However, we did two different things in our work: 
(1) the aforementioned techniques apply neural network as the underlying 
model while we follow the rationale proposed by some classic ranking 
model, i.e. the ranking score should be positively correlated with TF and 
inversely correlated with DL, to find the local optimum of the generalized 
ranking models.
(2) we aim to optimize MAP instead of NDCG and we proposed a simplified 
equation for calculating the difference of MAP if two documents are 
swapped in the ranking list which can make the analysis more efficiency. 
There is another work which indeed directly optimizes MAP called SVMMAP 
\cite{Yue:2007:SVM:1277741.1277790}. SVMMAP is actually another learning 
to ranking algorithm based on support vector machine. It performs 
optimization only on a working set of constraints which is extended with 
the most violated constraint at each optimization step. 
Taylor et al. \cite{Taylor:2006:OMR:1183614.1183698} used the cost 
analysis to predicate a family of BM25 ranking models. They however 
did not apply the gain analysis which has shown to be superior in 
our experiments.

