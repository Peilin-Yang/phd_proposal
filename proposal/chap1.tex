%
% This is Chapter 1 file (chap1.tex)
%
\chapter{Introduction}
The past decades have witnessed the tremendous success of World Wide Web. 
People all over the world can now access to publicly available 
information via commercial search engines such as Google or Baidu 
with great ease. According to the online 
statistics\footnote{http://www.internetlivestats.com/google-search-statistics/}, 
Google now (as of October 2016) can handle over 40,000 search queries 
every second on average, which translates to over 3.5 billion searches 
per day and 1.2 trillion searches per year worldwide. 
With such huge volume of search activities it is essential to make the 
search results of high quality in order to meet the users needs.

Information Retrieval (IR), usually used by academia in favor of its 
industrial counterpart search engine, is one of the most evolving fields 
and has drawn extensive attention in recent years.
Generally speaking, there are different types of IR systems targeting to meet 
different users' information needs. 
For end users of the system they do not necessarily know the details of 
how to design, how to implement the ranking algorithm of the system -- 
they just need to know how to use the system to meet their information needs. 
One such example is the contextual suggestion system 
\cite{udel:treccs2013, udel:treccs2014, udel:treccs2015, 
Yang:2013:OUP:2499178.2499191, Yang2015}.
On the other hand, an IR system could also be mainly for researchers. 
For IR researchers who are interested in understanding the IR ranking models, 
what they want is the IR system where they can modify the ranking model of 
the system more easily and quickly, test it, iterate to next round.
This proposal focuses on two IR systems which are both related long-term 
information retrieval task. The first one is contextual suggestion which is 
mainly for end users and the other one is ranking model performance upper 
bound analysis using unified evaluation system which is mainly for IR 
researchers. Specifically, we explorer the context and its impact for these 
two systems.

The word ``\textit{Context}'' is originally defined as 
``the set of circumstances or facts that surround a particular event, 
situation, etc.''
In this proposal context is studied in order to reveal and quantify 
its impact for different research problems. We will show more details 
in the following.

\section{Contextual Suggestion}
Another research endeavor is to provide better IR system for 
end users. This kind of system is different from the previous one since end 
users do not necessarily know about the implementation or design details of 
the system -- what they want is how to use the system to meet their information 
needs. Contextual suggestion is one of such example. 
The task of contextual suggestion is to recommend interesting venues to the 
users based on contextual information such as geographic location, 
temporal information and user's activity history. Traditionally context of 
this problem is often referred to the physical conditions of the users. 
This proposal proposes that user's long-term, static preferences should also 
be included in the context as it is the key condition we have to rely on.

There are two necessary steps to tackle the contextual suggestion problem 
and both of them rely on the context we have defined: 
(1) identify a set of candidates that satisfy the contextual 
requirements, e.g., places of interest that are close to a target place; 
(2) rank the candidate suggestions based on user's interests. 
User profiling is the key component to effectively rank candidate places 
with respect to a user's information need and this is the reason we include 
the user preference history into the context. 

In order to model use profile we first propose to leverage the 
category and description information about the places in user's activity 
history to construct user profiles \cite{udel:treccs2012}. 
The advantage of such approach is the ease of computation and the satisfactory 
results \cite{adriel:overview}. 
We further find that using category or description to build a user profile 
is not enough: category of places is too general to capture a user's 
underlying needs; while the text description of a place is too specific to 
be generalized to other places. 
In other studies 
\cite{udel:treccs2013,udel:treccs2014,udel:treccs2015,Yang:2013:OUP:2499178.2499191,Yang2015}
we leverage opinion, 
i.e. opinion ratings and the associated text reviews, to construct an 
opinionated user profile. By doing like this we aim to explain 
``why the user likes or dislikes the suggestion'' instead of simply recording 
``what places the user liked or dislike'' in the search history. 
The problem of this approach is that on-line opinions are notoriously skewed 
as only very small number of people post their opinions. 
To address this data sparsity challenge we propose to also include the 
opinions from similar users as the current user to construct the profile of 
current user. The assumption here is that users with similar ratings have 
the similar reasons of giving the rating. By modeling the candidate places 
in the similar fashion the similarity between user profile and candidates 
profile is used to rank the candidates. We tried different representations 
of the text reviews when modeling the profiles. We further apply 
Learning-to-Rank (LTR) method to the similarity scores for the ranking method. 
Experiment results on TREC collections and a self-crawled Yelp collection 
validate the effectiveness of the method.

\section{Unified IR Evaluation System}
The first problem we address is a long standing problem in evaluating the 
effectiveness of IR system\footnote{There are several aspects in IR system 
can be evaluated. In the proposal, it focuses on the evaluation of 
effectiveness of the system. Specifically only the effectiveness of the 
ranking model is investigated}. 
% Many different techniques can be applied to address the effectiveness of 
% the system. For example, Natural Language Processing (NLP) techniques \cite{Voorhees:1999:NLP:645857.669935,Mihalcea:2011:GNL:1984806}. 
% Topic Modeling \cite{Blei:2003:LDA:944919.944937,Hofmann:1999:PLS:312624.312649}.
% But most of the previous works target on the simple yet effective ranking 
% models which usually applied to the document index.
For a typical IR evaluation system the ideal case is to have a unified 
testing environment which is responsible for everything related 
to the evaluation process except the ranking model part. That said, everything 
including pre-processing and indexing the documents, generating the ranking 
list, evaluating the results, the choice of evaluation metrics and 
interpretation of the performance, all should be under the same settings 
if one's purpose is purely compare the effectiveness of different ranking models.
For this problem the unified testing environment is regarded as the context of 
the evaluation process. 

Apparently the context of this problem is the very basis of comparing the 
effectiveness of ranking models and thus should be carefully treated. 
Without this context researchers cannot make sound claim about 
their proposed models. Unfortunately, there is no such environment for the 
IR community. People continuously report different performances on the same 
baseline model \cite{Yang:2016:RSI:2970398.2970415} and this casts doubt on 
the real effectiveness of the proposed models.

In this proposal, two systems, namely VIRLab (Virtual IR Lab) 
\cite{Fang:2014:VWV:2600428.2611178} 
and RISE (Reproducible Information retrieval System Evaluation) 
\cite{Yang:2016:RSI:2970398.2970415} are proposed in order to offer 
a unified context to the IR community for standardization of comparing 
ranking models. The uniqueness and the advantage of these two systems is 
that they offer centralized and controlled IR evaluation systems which 
facilitate easy but trusted comparison of retrieval models. 
With the help of these systems (this proposal uses RISE) 
we are able to conduct a comprehensive reproducibility study for information 
retrieval models. 
In particular, we implement and evaluate more than 20 basic retrieval 
functions over 16 standard TREC collections. Experimental results allow us to 
make a few interesting observations. We first compare the evaluation results 
with those reported in the original papers, and find that the performance 
differences between the reproduced results and the original ones are small 
for majority of the retrieval functions. Among all the implemented functions, 
only one of them consistently generates worse performance than the one 
reported in the original paper. Moreover, we report the retrieval 
performance of all the implemented retrieval functions over all the 16 
TREC collections including recently released ClueWeb datasets. 
This is the first time of reporting such a large scale comparison 
of IR retrieval models. Such a comparison can be used as the performance 
references of the selected models.


\section{Boundary Theory of Bag-of-Terms Models}
With the unified IR evaluation system like VIRLab and RISE we have mentioned 
above we are able to compare ranking models with ease and trust. 
After a comprehensive comparison of the most widely used ranking models 
\cite{Yang:2016:RSI:2970398.2970415,Yang:2016:ERP:2970398.2970428} 
we find that the optimum performances of some models 
\cite{Robertson96okapiat3,Singhal:1996:PDL:243199.243206,
Zhai:2004:SSM:984321.984322,Amati:2002:PMI:582415.582416,
Fang:2005:EAA:1076034.1076116,Lv:2011:LTF:2063576.2063584,
He:2005:SDP:1076034.1076114}
are quite similar on several TREC collections. 
The commonalities of those models are: 
(1) all of them are based on bag-of-terms document representation assumption. 
That is, terms in the document are independent with each other and the 
occurrence (or absence) of one term does not affect the occurrence (or absence) 
of any other terms, and   
(2) the models consist of basic signals (statistics) such as 
Term Frequency (TF), Inverted Document Frequency (IDF), 
Document Length Normalization (DLN) and other collection statistics 
\cite{Fang:2004:FSI:1008992.1009004}. 
The commonalities could be viewed as the context of these ranking models 
since they are the theoretical foundation of these retrieval functions.
With this context some interesting questions here would be: 
it remains unclear whether we have reached the performance upper bound for 
such retrieval models. 
If so, what is the upper bound performance? If not, how can we do better?

To find the performance upper bound is quite challenging: although most of 
the IR ranking models deal with basic signals, how they combine the signals 
to compute the relevance scores are quite diverse due to different 
implementations of IR heuristics \cite{Fang:2004:FSI:1008992.1009004}. 
This kind of variants makes it difficult to generalize the analysis. 
Moreover, typically there are one or more free parameters in the ranking 
models which can be tuned via the training collections. 
These free parameters make the analysis more complicated.
We can simplify the problem and just focus on single-term queries 
and study how to estimate the performance bound for retrieval functions 
utilizing only basic ranking signals. 
With only one term in a query, many retrieval functions can be greatly 
simplified. For example, Okapi BM25 and Pivoted normalization functions have 
different implementations for the IDF part, but this part can be omitted in the 
functions for single-term queries because it would not affect the ranking of 
search results. All the simplified functions can then be generalized to a 
general function form for single-term queries. As a result, the problem of 
finding the upper bound of retrieval function utilizing basic ranking signals 
becomes that of finding the optimal performance of the generalized retrieval 
function. We propose to use cost/gain analysis to solve the problem 
\cite{export:132652,export:68133,export:81144}. 
As the estimated performance upper bound of simplified/generalized model 
is in general better than the existing ranking models, this finding 
provides the practical foundation of the potentially more effective 
ranking models for single term queries.

\section{Summary}

Previous studies in IR rarely separated the context apart from other 
components of a specific research problem. However, we argue that the 
context of different IR systems or components should be carefully treated 
and extensively studied as the impact of the context is big enough to 
influence the results some IR studies, e.g. the ones aforementioned above.

As of future work, we would further explorer in two directions. 
The first one is to quantify the impact of the context of unified 
IR evaluation system. There is no previous work on how much difference does 
the usage of different retrieval tools bring.
We hope to be the first to report on standardizing and quantifying the 
impact so that the IR community could be aware of such divergence and can 
better evaluate the contributions of using various tools. 
The second one is to provide more sound justification about the boundary 
theory of ranking model performance. Specifically, we want to extend the 
current analysis on the single term queries to multiple term queries. 
We also would like to try other method, e.g. explanatory analysis, 
to achieve the same goal.

The rest of the thesis is organized as follows. First, we discuss related 
work in Chapter 2. We describe our reproducibility study using VIRLab and 
RISE in Chapter 3. We explain how we use cost/gain analysis to find the 
performance upper bound for single term queries in Chapter 4. 
In Chapter 5, we investigate the problem of contextual suggestion and 
present our approaches. We then discuss future work in Chapter 6. 
Finally, we summarize the contributions of the thesis on Chapter 7.
