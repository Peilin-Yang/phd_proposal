%
% This is Chapter 1 file (chap1.tex)
%
\chapter{Introduction}
The past decades have witnessed the tremendous success of World Wide Web. 
People all over the world can now access to publicly available 
information via commercial search engines such as Google, Microsoft Bing 
with great ease. According to the online statistics 
\footnote{http://www.internetlivestats.com/google-search-statistics/}, 
Google now (as of October 2016) can handle over 40,000 search queries 
every second on average, which translates to over 3.5 billion searches 
per day and 1.2 trillion searches per year worldwide. 
With such huge volume of search activities it is essential to make the 
search results of high quality in order to meet the users needs.

Information Retrieval (IR), usually used by academia in favor of its 
industrial counterpart search engine, is one of the most evolving fields and 
has drawn extensive attention in recent years.
The primary goal of IR research is to improve the effectiveness or the 
efficiency or both of the textual retrieval system. 
There are many related works dedicated to this line of research already. 
For example, \textbf{Citation Here!!!!!} \cite{Singhal:1996:PDL:243199.243206}. 
However, there is few literatures dedicated to elaborate on the context 
of these research efforts.

The word ``\textit{Context}'' is originally defined as 
``the set of circumstances or facts that surround a particular event, situation, etc.'' 
In IR research the context always plays an crucial role and it is the 
fundamental basis of some very important IR research efforts. 
Without clearly defining of the context of each line of research we might get  
dim results and this is not a good sign for the whole IR community. 
Here we show three domains that extensively studied in this dissertation. 

For example, classic IR ranking models are mainly based on 
bag-of-terms document representation assumption and they mainly 
consist of statistics such as Term Frequency (TF), 
Inverted Document Frequency (IDF), 
Document Length Normalization (DLN) and other collection statistics. 
Here the bag-of-terms assumption 
and the commonly used statistics are the context of the ranking models. 
Another example comes from the IR evaluation. 
For evaluation of IR system the ideal case is we use a unified testing 
environment to assess the ranking models. 
The evaluation process is then purely based on the algorithms since the 
unified testing environment takes care of the possible processing stages, 
e.g. prepare the data and standardize the results. 
Here the ideal unified testing environment is the context of the evaluation 
process. 
For the third example we choose one of the IR recommendation system which is 
called contextual suggestion. For contextual suggestion the problem is to 
recommend interesting venues to the users based on contextual information 
such as geographic location, temporal information and user's activity history. 
Context again, as the name of the problem shows, highlights this direction 
of research effort. 
Previous studies in IR rarely separated the context apart from other 
components of IR research problem. However, we argue that the context of 
different IR systems or components should be carefully treated and extensively 
studied as the impact of the context is big enough to question the foundation 
of some IR studies.
For example, the evaluation streamline is one of the key 
components of IR system where different ranking approaches can be 
easily compared. Moreover, there are many other web applications which 
are highly related to the IR system. One of such domain is recommendation 
system where researchers tried their best to incorporate IR techniques 
with this area hoping to satisfy users different needs.


The primary goal of IR research is to effectively address user's 
information needs such as search via text queries or recommendation 
based on historical activities. 

Opinions are also used to generate personalized and high 
quality summaries of the suggestions. 