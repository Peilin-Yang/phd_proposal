%
% This is Abstract
%
Information Retrieval (IR) is one of the most evolving research fields 
and has drawn extensive attention in recent years. 
In IR research, context plays a crucial role and it is the fundamental basis 
of some very important IR research efforts.
In this proposal, we identify the contexts of three different domains 
of IR researches and investigate their impacts. 

The first scenario is the unified IR evaluation system. 
Previously, the typical work-flow of proposing a new IR ranking model is to 
implement the algorithm using the most convenient tool (e.g. Indri) of the researcher's choice. 
The ranking model is then evaluated against standard 
data collections and the performances are compared with commonly used 
baselines (e.g. BM25) and the advantage of the proposed model is shown. 
The problem of such methodology is that different tools and experimental 
settings may result in different results for the same model and thus cast 
doubts on the real effectiveness of the proposed ranking models. 
We have monitored the adverse effect brought by the aforementioned problem and 
explore the usage of the standardized IR evaluation environment. 
VIRLab as our first attempt to alleviate the problem. It provides an 
easy-to-use IR evaluation system where user of the system can implement 
her/his our model without considering too much about the underlying framework. 
% An Privacy Preserve Evaluation (PPE) system based on commercial data 
% collection enables the API based IR evaluation without the redistribution 
% of data collections due to privacy/license issue.
A web based Reproducible IR Evaluation System (RISE) is the first ever known 
system that users of the system can collaborate with each other and thus make 
the normalization of baseline models much more easier and thus can be trusted 
by future researchers. A large scale experiment based on RISE serves as the 
the performance reference of most widely used IR models.

Another example is the boundary theory of ranking model performance. 
The context of this line of research is the bag-of-terms representation of 
the document assumption and the widely used statistics, e.g. Term Frequency 
(TF) and Inverted Document Frequency (IDF) 
that the ranking model consists of. We first compare the optimal 
performances of state-of-the-art ranking models and find the optimums 
are similar for those models even the underlying theories are different. 
To dig it deeper we use the cost/gain analysis which is commonly used in 
learning-to-rank (LTR) regime to find the optimum of single term queries for 
a family of ranking models that share similar strategy of combining key 
signals. The result shows that although the performances of state-of-the-art 
ranking models are quit close to the practical optimum there is still some 
room for improvement.

The third domain is to investigate the usage of opinion to contextual 
suggestion. It is commonly agreed that how to model user profile is the key 
to the solution. We first model the user profile using the venue's category 
and description of user activity history. We further improve the 
method by leveraging the opinions from user's activity history to model the 
user profile. Such methodology naturally utilizes the rich text associated 
with the users which naturally makes the problem as a retrieval problem. 
By modeling the candidate suggestions in the similar fashion, 
the similarities between the candidates and the user profile are used to score 
the candidates. Experiments on TREC collections and a 
Yelp data collection reveals the advantage of our method.

For the future work, there are mainly two directions we would like to explore more deeper. 
The first one is to quantify the impact of the context of unified 
IR evaluation system. There is no previous work on how much difference does 
the usage of different retrieval tools bring.
We hope to be the first to report on standardizing and quantifying the 
impact so that the IR community could be aware of such divergence and can 
better evaluate the contributions of using various tools. 
The second one is to provide more sound justification about the boundary 
theory of ranking model performance. Specifically, we want to extend the 
current analysis on the single term queries to multiple term queries. 
We also would like to try other method, e.g. explanatory analysis, 
to achieve the same goal.















