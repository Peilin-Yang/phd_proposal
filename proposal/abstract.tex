%
% This is Abstract
%
Information Retrieval (IR) is one of the most evolving research fields 
and has drawn extensive attention in recent years. 
Typically, there are different types of IR systems targeting to meet 
different users' information needs. 
For IR researchers they want an IR system which is more 
fundamental, transparent and touchable. Such system could offer the researchers 
the ability to easily modify, test and iterate their hypothesis. 
On the other hand, an IR system could also be mainly for end users. 
Users of the system do not necessarily know the details of how to design, 
how to implement the ranking algorithm of the system -- they just need to know 
how to use the system to meet their information needs.

In order to make better IR systems that are satisfied for both researchers 
and end users, in our work we explorer the impact of context in two specific 
long-term IR related tasks -- ranking model performance upper bound analysis 
using unified evaluation system and contextual suggestion.
 
Previously, the typical work-flow of proposing a new IR ranking model is to 
implement the algorithm using the most convenient tool (e.g. Indri) of the 
researcher's choice. 
The ranking model is then evaluated against standard data collections and 
the performances are compared with commonly used baselines (e.g. BM25 \cite{Robertson96okapiat3}) 
and the advantage of the proposed model is presented. 
The problem of such methodology is that different tools and experimental 
settings may result in different results for the same model \cite{Yang:2016:RSI:2970398.2970415} 
and this is a long standing problem of IR community for over 20 years \cite{Armstrong:2009:IDA:1645953.1646031}. The fact casts doubts on the real 
effectiveness of the proposed ranking models and it is essential for the 
IR community to have a unified evaluation system so that ranking models 
can be compared without considering the details -- usually other components 
of an IR system, e.g. preprocessing of documents, choice of evaluation metrics. 
In our work we propose two unified IR evaluation systems to alleviate this problem. 
VIRLab as our first attempt. It provides an easy-to-use IR evaluation system 
where user of the system can implement her/his our model without considering 
too much about the underlying framework.
A web based Reproducible IR Evaluation System (RISE) is the first ever known 
system that users of the system can collaborate with each other and thus make 
the normalization of baseline models much more easier and thus can be trusted 
by future researchers. A large scale experiment based on RISE serves as the 
the performance reference of most widely used IR models.

Another example is the boundary theory of ranking model performance. 
The context of this line of research is the bag-of-terms representation of 
the document assumption and the widely used statistics, e.g. Term Frequency 
(TF) and Inverted Document Frequency (IDF) 
that the ranking model consists of. We first compare the optimal 
performances of state-of-the-art ranking models and find the optimums 
are similar for those models even the underlying theories are different. 
To dig it deeper we use the cost/gain analysis which is commonly used in 
learning-to-rank (LTR) regime to find the optimum of single term queries for 
a family of ranking models that share similar strategy of combining key 
signals. The result shows that although the performances of state-of-the-art 
ranking models are quit close to the practical optimum there is still some 
room for improvement.

The third domain is to investigate the usage of opinion to contextual 
suggestion. It is commonly agreed that how to model user profile is the key 
to the solution. We first model the user profile using the venue's category 
and description of user activity history. We further improve the 
method by leveraging the opinions from user's activity history to model the 
user profile. Such methodology naturally utilizes the rich text associated 
with the users which naturally makes the problem as a retrieval problem. 
By modeling the candidate suggestions in the similar fashion, 
the similarities between the candidates and the user profile are used to score 
the candidates. Experiments on TREC collections and a 
Yelp data collection reveals the advantage of our method.

For the future work, there are mainly two directions we would like to explore more deeper. 
The first one is to quantify the impact of the context of unified 
IR evaluation system. There is no previous work on how much difference does 
the usage of different retrieval tools bring.
We hope to be the first to report on standardizing and quantifying the 
impact so that the IR community could be aware of such divergence and can 
better evaluate the contributions of using various tools. 
The second one is to provide more sound justification about the boundary 
theory of ranking model performance. Specifically, we want to extend the 
current analysis on the single term queries to multiple term queries. 
We also would like to try other method, e.g. explanatory analysis, 
to achieve the same goal.
