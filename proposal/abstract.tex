%
% This is Abstract
%
Information Retrieval (IR) is one of the most evolving research fields 
and has drawn extensive attention in recent years. 
Typically, there are different types of IR systems targeting to meet 
different users' information needs. We are more interested in two kinds of 
IR systems: (1) the system for IR researchers who are interested in 
understanding the IR ranking models, (2) the contextual suggestion system 
mainly for end users.
In order to make better aforementioned IR systems that are satisfied 
for both researchers and end users, in our work we explorer the impact of 
context in two specific long-term IR related tasks -- ranking model 
performance upper bound analysis using unified evaluation system 
and contextual suggestion.

A long standing problem of IR community is that newly proposed ranking models 
are implemented and evaluated using various systems of researchers' choices 
and such choices lead to the fact that different researchers reported 
different results for the same baseline model \cite{Yang:2016:RSI:2970398.2970415}. 
The context -- IR systems used by researchers, 
is apparently the cause of such problem. This fact casts doubts on the real 
effectiveness of the proposed ranking models and we argue that it is 
essential for the IR community to have a unified evaluation system so that 
ranking models can be compared without considering the details -- usually 
other components of an IR system, e.g. preprocessing of documents, choice of 
evaluation metrics. 
In our work we propose two unified IR evaluation systems to alleviate this problem. 
VIRLab as our first attempt. It provides an web based IR evaluation system 
where user of the system can implement her/his our model and get it 
automatically evaluated.
Another system RISE (Reproducible IR System Evaluation) is proposed later 
to reflect the other endeavor from us. 
RISE is the first ever known system that users of the system 
can collaborate with each other and thus make the normalization of baseline 
models much more easier and thus can be trusted by future researchers. 
A large scale reproducibility experiment based on RISE serves as the the 
performance reference of most widely used IR models.

Based on the reproducibility experiment results of using RISE we find that 
the optimum performances of several TREC collections do not improve a lot 
for over 20 years if bag-of-terms ranking model is used. 
This introduces another interesting question: does it exist the performance 
upper bound for bag-of-terms models? To answer this question we extensively 
investigate the context -- the bag-of-terms document representation 
assumption and the statistics like term frequency and inverted document 
frequency the models usually play with.
We first find that for single term queries several ranking models can be 
transformed to a simplified model. 
We then apply the cost/gain analysis which is commonly used in 
learning-to-rank (LTR) domain to find the optimum of single term queries 
for this simplified model.
The result shows that although the performances of state-of-the-art 
ranking models are quit close to the practical optimum there is still some 
room for improvement.

For contextual suggestion system it is commonly agreed that how to model user 
profile is the key to the solution. We argue that user's preference which is 
usually modeled as a static or long term factor should also be considered as 
part of the context of this problem. 
Our first approach is to model the user profile using the venue's category 
and description from user's activity history. We further improve the 
method by leveraging the opinions from user's activity history to model the 
user profile. Such methodology utilizes the rich text associated 
with the users which naturally fits the problem into IR domain. 
By modeling the candidate suggestions in the similar fashion, 
the similarities between the candidates and the user profile are used to 
score the candidates. Experiments on TREC collections and a 
Yelp data collection confirm the advantage of our method.

For the future work, there are mainly two directions we would like to explore more. 
The first one is to quantify the impact of the context of unified 
IR evaluation system. There is no previous work on how much difference does 
the usage of different retrieval tools bring.
We hope to be the first to report on standardizing and quantifying the 
impact so that the IR community could be aware of such divergence and can 
better evaluate the contributions of using various tools. 
The second one is to provide more sound justification about the boundary 
theory of ranking model performance. Specifically, we want to extend the 
current analysis on the single term queries to multiple term queries. 
We also would like to try other method, e.g. explanatory analysis, 
to achieve the same goal.
