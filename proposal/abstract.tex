%
% This is Abstract
%
Information Retrieval (IR) is one of the most evolving research fields 
and has drawn extensive attention in recent years. 
In IR research, context plays a crucial role and it is the fundamental basis 
of some very important IR research efforts.
% For example, classic IR ranking models are mainly based on 
% bag-of-terms document representation assumption and they mainly 
% consist of statistics such as Term Frequency (TF), 
% Inverted Document Frequency (IDF), 
% Document Length Normalization (DLN) and other collection statistics. 
% Here the bag-of-terms assumption 
% and the commonly used statistics are the context of the ranking models. 
% Another example comes from the IR evaluation. 
% For evaluation of IR system the ideal case is we use a unified testing 
% environment to assess the ranking models. 
% The evaluation process is then purely based on the algorithms since the 
% unified testing environment takes care of the possible processing stages, 
% e.g. prepare the data and standardize the results. 
% Here the ideal unified testing environment is the context of the evaluation 
% process. 
% For the third example we choose one of the IR recommendation system which is 
% called contextual suggestion. For contextual suggestion the problem is to 
% recommend interesting venues to the users based on contextual information 
% such as geographic location, temporal information and user's activity history. 
% Context again, as the name of the problem shows, highlights this direction 
% of research effort. 
% Previous studies in IR rarely separated the context apart from other 
% components of IR research problem. However, we argue that the context of 
% different IR systems or components should be carefully treated and extensively 
% studied as the impact of the context is big enough to question the foundation 
% of some IR studies.
% In this proposal, we identify the contexts of three different domains 
% of IR researches and investigate their impacts. 

The first scenario is the unified IR evaluation system. 
Previously, the typical work-flow of proposing a new IR ranking model is to 
implement the algorithm using the most convenient tool (e.g. Indri, Lucene, 
Terrier) of the researcher's choice. 
The ranking model is then evaluated against standard 
data collections and the performances are compared with commonly used 
baselines, e.g. BM25 ranking model. The problem of such methodology is that 
different tools and experimental settings of researcher's choice, i.e. the 
context of such approach varies and thus cast doubts on the real effectiveness 
of the proposed ranking models. 
We have monitored the adverse effect brought by the aforementioned problem and 
explore the usage of the standardized IR evaluation environment. 
VIRLab as our first attempt to alleviate the problem. It provides an 
easy-to-use IR evaluation system where user of the system can implement 
her/his our model without considering too much about the underlying framework. 
% An Privacy Preserve Evaluation (PPE) system based on commercial data 
% collection enables the API based IR evaluation without the redistribution 
% of data collections due to privacy/license issue.
A web based Reproducible IR Evaluation System (RISE) is the first ever known 
system that users of the system can collaborate with each other and thus make 
the normalization of baseline models much more easier and thus can be trusted 
by future researchers. A large scale experiment based on RISE serves as the 
the performance reference of most widely used IR models.

Another example is the boundary theory of ranking model performance. 
The context of this line of research is the bag-of-terms representation of 
the document assumption and the widely used statistics, e.g. Term Frequency 
(TF), Inverted Document Frequency (IDF), Document Length Normalization(DLN), 
that the ranking model consists of. We first compare the optimal 
performances of state-of-the-art ranking models and find the optimums 
are similar for those models even the underlying theories are different. 
To dig it deeper we use the cost/gain analysis which is commonly used in 
learning-to-rank (LTR) regime to find the optimum of single term queries for 
a family of ranking models that share similar strategy of combining key 
signals. The result shows that although the performances of state-of-the-art 
ranking models are quit close to the practical optimum there is still some 
room for improvement.

The third domain is to investigate the usage of opinion to contextual 
suggestion. In this retrieval problem we are the first ones leveraging the 
opinions from not only the user's activity history but also the opinions from 
similar users to model the user profile. 
Such methodology naturally combines the content based filtering and 
collaborative filtering into one model. By modeling the candidate suggestions 
in the similar fashion, the similarities between the candidates and the user 
profile are used to score the candidates. Opinions are also used to generate 
personalized and high quality summaries of the suggestions. Experiments on 
TREC collections and a Yelp data collection reveals the advantage of our method.

For the future work, there are mainly two directions we would like to explore more. 
First is to further investigate the impact of the context of unified 
IR evaluation system. We would like to explorer more about the impact 
introduced by using different tools with choices of different components 
of the evaluation system. We hope the findings are two folds: 
(1) the impact brought by the different choices of the components of IR 
evaluation system, and (2) standardize and quantify the impact so that the IR 
community could be aware of such divergence and can better evaluate the 
contributions of using various tools. 
Second is to provide more sound justification about the boundary theory 
of ranking model performance. Specifically, we want to extend the current 
analysis on the single term queries to multiple term queries. We also would 
like to try other method, e.g. explanatory analysis, to achieve the same goal.















