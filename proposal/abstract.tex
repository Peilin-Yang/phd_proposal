%
% This is Abstract
%
Information Retrieval (IR) is one of the most evolving research fields 
and has drawn extensive attention in recent years. 
There are different types of IR systems targeting to meet 
different users' information needs. Two IR systems are of the most interest 
in our works: (1) the contextual suggestion system that shows interesting venues to end users, (2) the evaluation system for IR researchers who are 
interested in understanding the IR ranking models.
In order to make better IR systems that are satisfied by both researchers 
and end users, in our work we explorer the impact of 
context in two specific long-term IR related tasks -- contextual suggestion 
and ranking model performance upper bound analysis using unified evaluation 
system.

For contextual suggestion system it is commonly agreed that successful 
modeling user profile is the key to the solution. We argue that user's 
long-term preference should also be considered as part of the context of 
this problem. 
Our first approach is to model the user profile using the venue's category 
and description from user's activity history. We further improve the 
method by leveraging the opinions from user's preference to model the 
user profile. Such methodology utilizes the rich text associated 
with the users which naturally fits the problem into IR domain. 
By modeling the candidate suggestions in the similar fashion, 
the similarities between the candidates and the user profile are used to 
score the candidates. Experiments on TREC collections and a Yelp data 
collection confirm the advantage of our method.

A long standing problem of IR community is that newly proposed ranking models 
are implemented and evaluated using various systems of researchers' choices 
and such choices lead to the fact that different researchers reported 
different results for the same baseline model. 
The context -- the IR evaluation system used by researcher, 
is apparently the cause of such problem. This fact casts doubts on the real 
effectiveness of the proposed ranking models and we argue that it is 
essential for the IR community to have a unified evaluation system so that 
ranking models can be compared without considering the details -- usually 
other components of an IR system, e.g. preprocessing of documents 
or the choice of evaluation metrics. 
In our work we propose two unified IR evaluation systems to alleviate this problem. 
VIRLab as our first attempt provides a web based IR evaluation system 
where the users of the system can implement their own models and get them 
automatically evaluated.
Another system RISE (Reproducible IR System Evaluation) is proposed in order 
to further improve VIRLab.
To the best of our knowledge, RISE is the first ever known system that users 
of the system can collaborate with each other and thus make the 
normalization of baseline models much more easier and thus can be trusted 
by future researchers. 
With the help of RISE a large scale reproducibility experiment is deployed 
and the comprehensive results serve as the the performances reference of 
most widely used IR models.

Based on the reproducibility experiment results of using RISE we find that 
the optimum performances of bag-of-terms ranking models do not improve a lot 
on TREC collections for over 20 years. 
This introduces another interesting question: does it exist the performance 
upper bound for bag-of-terms models? To answer this question we extensively 
investigate the context -- the bag-of-terms document representation 
assumption and the statistics like term frequency and inverted document 
frequency with which the models usually play.
We first find that for single term queries several ranking models can be 
transformed to a simplified model. We then apply the cost/gain analysis which 
is commonly used in learning-to-rank (LTR) domain to find the optimum of 
single term queries for this simplified model.
The result shows that although the performances of state-of-the-art 
ranking models are quit close to the practical optimum there is still some 
room for improvement.

For the future work, there are mainly two directions we would like to explore more. 
The first one is to quantify the impact of the context of unified 
IR evaluation system. There is no previous work on how much difference does 
the usage of different retrieval tools bring.
We hope to be the first to report on standardizing and quantifying the 
impact so that the IR community could be aware of such divergence and can 
better evaluate the contributions of using various tools. 
The second one is to provide more sound justification about the boundary 
theory of ranking model performance. Specifically, we want to extend the 
current analysis on the single term queries to multiple term queries. 
We also would like to try other method, e.g. explanatory analysis, 
to achieve the same goal.
